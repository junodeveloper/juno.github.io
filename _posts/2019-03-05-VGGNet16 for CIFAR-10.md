# VGGNet16 for CIFAR-10

논문(링크)의 16-layers 구조를 CIFAR-10 이미지 분류에 적용해보았다.



## 1. 단순 구현

모든 컨벌루션 레이어는 논문과 동일하고, FC 레이어의 개수는 3개에서 2개로 줄였다. 그리고 FC의 노드 개수를 4096개에서 512개로 줄였다. 입력 이미지의 사이즈가 32x32이므로 5번까지 pooling을 거쳐도 괜찮다. (stride=2) batch size는 128로 주었다.

그 외에 별다른 dropout이나 regularization은 적용하지 않고 학습했을 때에는 2000번 이상의 iteration으로도 학습이 전혀 되지 않았다. (예측률이 기댓값인 0.1에 불과)

## 2. dropout

dropout을 추가해도, 거의 20000번 이상의 iteration으로도 학습이 전혀 안 되었다. (rand() % 10 이랑 동급;)

## 3. Batch normalization

BN을 마지막 FC 레이어를 제외한 모든 레이어에 추가해주었다. 2000번의 iteration만으로 60%대의 정확도를 찍었다.



## 소감

최근에는 BN이 거의 필수적으로 사용된다고 하던데, 왜 그런지를 몸소 실감할 수 있었다. BN을 사용하지 않으면 VGGNet과 같은 deep net이 아예 학습이 안되는 상황도 발생할 수 있다는 걸 깨달았다.



Iteration 2000번까지는 이전에 구현한 BasicCNN과 정확도가 비슷하게 나타났다. (대략 60% 초반)

 지금은 20000번 돌려보고 있다.